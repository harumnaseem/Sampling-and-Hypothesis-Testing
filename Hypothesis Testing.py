# -*- coding: utf-8 -*-
"""21100118.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PpHwbLjSVd5jp4sOTYVxaeIr4ZPTe7PF

# Sampling and Hypothesis Testing
"""

#Libraries
import numpy as np 
import pandas as pd 
from matplotlib import pyplot as plt
from sklearn import preprocessing
from scipy.stats import ttest_1samp
from prettytable import PrettyTable
import os
pd.options.mode.chained_assignment = None

"""# Visa Shopping

Visa shopping: It is where someone would apply in a specific consulate in order to have higher odds of getting the visa or getting a higher tier visa in general. The idea itself seemed to have a lot of rumours around it and the only way to consider it a myth/reality is by doing proper analysis over real data. Luckily, the data is here.

# Your job is to appropriately sample and run hypothesis testing to prove/disprove whether Visa-shopping is real.

We are trying to figure out the best consulates to apply for to have the highest odds for receiving a uniform visa in general, or a multiple entry visa (MEV) in particular.
"""

df = pd.read_csv("2018-data-for-consulates.csv")
df.head()

"""# Data Cleaning:

It is a good technique to strip column names of trailing white spaces and "\n" characters. Do that in the cell provided below.
- Rename columns with white spaces for e.g. "Schengen State': 'sch_state'"
- Look into df.strip()
"""

# Replacing empty spaces with underscore in the column names
# Getting rid of trailing white spaces
# Stripping \n from the column names
df.columns = df.columns.str.strip('\n')
df.columns = df.columns.str.replace(' ', '_')
df.columns = df.columns.str.strip()
df.head()

"""We are not interested in Airport Transit Visas (ATV). Drop all the columns involving ATVs for e.g. Multiple ATVs issued, ATVs not issued,"""

#Removing all columns which have ATV mentioned in them
df = df[df.columns.drop(list(df.filter(regex='ATV')))]
df.head()

# Renaming column names
df = df.rename(columns={'Schengen_State': 'sch_state','Consulate' :'consulate', 'Country_where_consulate_is_located': 'consulate_country', 'Uniform_visas_applied_for' : 'uv_applied', 'Total__uniform_visas_issued_(including_MEV)_' : 'total_uv+mev_issued', 'Multiple_entry_uniform_visas_(MEVs)_issued': 'mev_issued', 'Share_of_MEVs_on_total_number_of_uniform_visas_issued': 'shared_mev_issued', 'Total_LTVs_issued' : 'LTVs_issued', 'Uniform_visas_not_issued' : 'uv_unissued', 'Not_issued_rate_for_uniform_visas': 'unissued_uv_rate'})
df.head()

"""# Missing Values

- Drop the rows where Schengen State is missing.
- Replace other missing values with zero.
- Print the summary for missing values after cleaning.
"""

# Figuring out how many null values does each column have
df.isnull().sum()

# Dropping all rows which have a missing value in the Schengen State
df = df[df['sch_state'].notnull()]
df.head()

# Verifying if all the null values in Schengen State have been removed
df.isnull().sum()

# Replacing all the other null values (in all other columns) with zero
# df.astype('str')
for col in df.columns:
    df[col] = df[col].astype(str)
    if col != "sch_state" and col != "consulate_country" and col != "consulate":     
        df[col] = df[col].replace("nan", "0")

# Verifying if all the null values in all the remaining columns have been replaced
df.isnull().sum()

df.dtypes

for x in df.columns:
    df[x] = df[x].str.replace(',', '')
    df[x] = df[x].str.replace('%', '')
df.head()

for col in df.columns:
    if col != "sch_state" and col != "consulate_country" and col != "consulate" and col != 'shared_mev_issued' and col != 'unissued_uv_rate':
        df[col] = df[col].astype(int)
    if col == 'shared_mev_issued' or col == 'unissued_uv_rate':
        df[col] = df[col].astype(float)
df.dtypes

"""- Add columns of "decisions" and "rejection_rate"
- Decisions is total number of decisions taken - sum of visas issued, LTV's issued, Rejected
- Rejection rate is what percentage of *decisions* are rejected
"""

# Calculating the decisions value and adding another column to represent it
decisions = []
for index, row in df.iterrows():
    value = row['total_uv+mev_issued'] + row['LTVs_issued'] + row['uv_unissued'] 
    decisions.append(value)
df['decisions'] = decisions
df.head()

# Calculating the rejection rate and adding another column to represent it 
rejection_rate = []
for index, row in df.iterrows():
    if(row['decisions'] == 0):
        rejection_rate.append(0)
    else:
        value = row['uv_unissued'] * 100.0 / row['decisions']
        rejection_rate.append(value)
df['rejection_rate'] = rejection_rate
df.head()

# There are some rows in which mev_issued are greater than total_uv+mev_issued which isn't possible 
# because if 10 people have issued the mev then the total_uv_mev_issued cannot be less than 10 because we know that 10 people have issued mev
# In this case, shared_mev_issued was greater than 100% too which can never happen. 
df = df[df['total_uv+mev_issued'] >= df['mev_issued']]
df.head()

"""# Hypothesis Testing

- We will be picking three countries.
- The three countries are Egypt, India and Japan.
- You need to test whether in there is a better _chance_ of getting a visa if you apply to a particukar consulate within that country or is it by chance.
- Add subheadings for each country

In order to answer this question correctly, we need to define the population and the sample we will use:
1. Population: Total number of applicants/rejects in all the country.
2. Sample: Total number of applicants/rejects in all the targeted consulates.
The null hypothesis, which is a prediction that there is no significant difference between a specific consulate over all other consulates and that these numbers could have happened out naturally due to fluctations in application qualities received.
In order to proceed with this, we need to make a rather **BIG** assumption, that all applications to all consulates have the same *average* quality or that they came from the same pool/population.

*Does this assumption make sense?*

_Write why or why not?_

*Write here*

No because there can be many other confounding factors* so we cannot assume that there will be equal likelyhood for every consulate and therefore making this assumption fals

* an outside influence that changes the effect of a dependent and independent variable

Yes because this assumption is necessary for us to be able to make any comparisons. We must assume that there is no difference (in an explicit sense) between the population and the sample we have. Without this assumption, we cannot test our hypothesis.

#  We can conduct this using univariate T-test.
You will use the scipy package.
This links will help you get started:
    - https://towardsdatascience.com/hypothesis-testing-in-machine-learning-using-python-a0dc89e169ce
    
    
You need to calculate the test statistic and choose an appropriate p-value. Mention why you think this is an appropriate p-value. Once you have the test statistic, you need to sample and test your hypothesis.

For each country:
   - Plot a piechart showing the consulates with the highest number of decisions.
   - You will be running your hypothesis tests w.r.t three attributes
       - Rejection rate (if you have a weak visa application)
       - Multi-Entry Visa Share (if you have a strong visa application)
       - Weighted Score: You are required to calculate a normalized engineered feature
           - (1-rejection_rate) * mevs_share
           - Normalize this score

You are required to submit an analysis of *EACH* country why you think your hypothesis were rejected/accepted. You can include geo-political factors in your report as well.

#  Egypt
"""

df1 = df[df['consulate_country'] == 'EGYPT'] 
df1.head()

"""# Egypt: Pie chart"""

# doing groupby so i can make piecharts using it
df1b = df1.groupby(['sch_state']).sum()
df1b

# making piechart
df1b.plot.pie(y='decisions', figsize=(50,30), fontsize = 14)

df1c = df1.groupby(['consulate']).sum()
df1c

df1c.plot.pie(y='decisions', figsize=(10,30), fontsize = 14)

df1.reset_index(drop = True, inplace = True)
df1.head()

"""# Egypt: Hypothesis testing with respect to Rejection Rate"""

# Adding the accepted column to my egypt dataframe
Accepted = []
for index, row in df1.iterrows():
    value = row['total_uv+mev_issued'] + row['LTVs_issued']
    Accepted.append(value)
df1['accepted'] = Accepted
df1.head()

# Dropping the consulate_country column because its of no use
df1 = df1.drop(['consulate_country'], axis = 1)
df1.head()

statisticalValue = (df1['rejection_rate']/100).mean()
print('Statistical value is ',statisticalValue)

# Making lists of 0s and 1s based on what the accepted and rejected (uv_unissued) values are
# 0 -> accepted
# 1 -> rejected
FinalList = [] 
for index, row in df1.iterrows():
    numberOfAccepted = row['accepted']
    numberOfRejected = row['uv_unissued']
    arrSize = numberOfAccepted + numberOfRejected
    arr = []
    for i in range(numberOfAccepted):
        arr.append(0)
    for i in range(numberOfRejected):
        arr.append(1)
    FinalList.append(arr)

# Running t test
table = PrettyTable()

table.field_names = ["sch_stat","consulate", "Status", "P-Value", "Statistical Value"]
lengthOfFinalList = len(FinalList)
for i in range(lengthOfFinalList):
    test = FinalList[i]
    tset, pvalue = ttest_1samp(test, statisticalValue)
    if pvalue < 0.05/2:
        table.add_row([df1.loc[i, 'sch_state'],df1.loc[i, 'consulate'], 'x', pvalue, tset])
    else:
        table.add_row([df1.loc[i, 'sch_state'],df1.loc[i, 'consulate'], 'o', pvalue, tset])
key = PrettyTable()
key.field_names = ['symbol', 'meaning']
key.add_row(['x', 'Null hypothesis rejected'])
key.add_row(['o', 'Null hypothesis accepted'])
print(key)
print(table)

"""# Egypt: Hypothesis testing with respect to Multi-Entry Visa Share"""

statisticalValue = (df1['shared_mev_issued']/100.0).mean()
print('Statistical value is ',statisticalValue)

# Adding the non MEV column to my egypt dataframe
nonMEV = []
for index, row in df1.iterrows():
    value = row['total_uv+mev_issued'] - row['mev_issued']
    nonMEV.append(value)
df1['nonMEV'] = nonMEV
df1.head()

# Making lists of 0s and 1s based on what the MEV issued and nonMEV values are
# 1 -> MEV
# 0 -> nonMEV
FinalList = [] 
for index, row in df1.iterrows():
    numberOfAccepted = row['mev_issued']
    numberOfRejected = row['nonMEV']
    arrSize = numberOfAccepted + numberOfRejected
    arr = []
    for i in range(numberOfAccepted):
        arr.append(1)
    for i in range(numberOfRejected):
        arr.append(0)
    FinalList.append(arr)

# Running t test
table = PrettyTable()

table.field_names = ["sch_stat","consulate", "Status", "P-Value", "Statistical Value"]
lengthOfFinalList = len(FinalList)
for i in range(lengthOfFinalList):
    test = FinalList[i]
    tset, pvalue = ttest_1samp(test, statisticalValue)
    if pvalue < 0.05/2:
        table.add_row([df1.loc[i, 'sch_state'],df1.loc[i, 'consulate'], 'x', pvalue, tset])
    else:
        table.add_row([df1.loc[i, 'sch_state'],df1.loc[i, 'consulate'], 'o', pvalue, tset])
key = PrettyTable()
key.field_names = ['symbol', 'meaning']
key.add_row(['x', 'Null hypothesis rejected'])
key.add_row(['o', 'Null hypothesis accepted'])
print(key)
print(table)

"""# Egypt: Hypothesis testing with respect to Weighted Score"""

weightedScore = []
for index, row in df1.iterrows():
    value = (1- (row['rejection_rate']/100.0))*(row['shared_mev_issued']/100.0)
    weightedScore.append(value)
df1['weighted_score'] = weightedScore

df1

x = (df1.loc[:,['weighted_score']]).values
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
df1['weighted_score'] = pd.DataFrame(x_scaled)

df1

statisticalValue = df1['weighted_score'].mean()
statisticalValue

accepted3 = []
rejected3 = []
for index, row in df1.iterrows():
    value = row['weighted_score']
    accepted3.append(value*row['decisions'])
    rejected3.append((1-value)*row['decisions'])

df1['accepted3'] = accepted3
df1['rejected3'] = rejected3
df1['accepted3'] = df1['accepted3'].astype(int)
df1['rejected3'] = df1['rejected3'].astype(int)
df1.head()

FinalList = [] 
for index, row in df1.iterrows():
    numberOfAccepted = row['accepted3']
    numberOfRejected = row['rejected3']
    arr = []
    for i in range(numberOfAccepted):
        arr.append(1)
    for i in range(numberOfRejected):
        arr.append(0)
    FinalList.append(arr)

# Running t test
table = PrettyTable()

table.field_names = ["sch_state","consulate", "Status", "P-Value", "Statistical Value"]
lengthOfFinalList = len(FinalList)
for i in range(lengthOfFinalList):
    test = FinalList[i]
    tset, pvalue = ttest_1samp(test, statisticalValue)
    if pvalue < 0.05/2:
        table.add_row([df1.loc[i, 'sch_state'],df1.loc[i, 'consulate'], 'x', pvalue, tset])
    else:
        table.add_row([df1.loc[i, 'sch_state'],df1.loc[i, 'consulate'], 'o', pvalue, tset])
key = PrettyTable()
key.field_names = ['symbol', 'meaning']
key.add_row(['x', 'Null hypothesis rejected'])
key.add_row(['o', 'Null hypothesis accepted'])
print(key)
print(table)

"""# Egypt: Final Analysis

For the schegian states which have rejected the null hypothesis, a factor may be that the Egyptian foreign affairs are becoming more strict when it comes to visas, mainly due to visa shopping.
Reference link:https://www.schengenvisainfo.com/news/egypt-urges-nationals-to-comply-with-schengen-travel-rules/

# India
"""

df2 = df[df['consulate_country'] == 'INDIA'] 
df2.head()

"""# India: Pie Chart"""

# doing groupby so i can make piecharts using it
df2b = df2.groupby(['sch_state']).sum()
df2b

# making piechart
df2b.plot.pie(y='decisions', figsize=(52,40), fontsize = 14)

df2c = df2.groupby(['consulate']).sum()
df2c

df2c.plot.pie(y='decisions', figsize=(15,30), fontsize = 14)

df2.reset_index(drop = True, inplace = True)
df2.head()

"""# India: Hypothesis testing with respect to Rejection Rate"""

# Adding the accepted column to my india dataframe
Accepted = []
for index, row in df2.iterrows():
    value = row['total_uv+mev_issued'] + row['LTVs_issued']
    Accepted.append(value)
df2['accepted'] = Accepted
df2.head()

# Dropping the consulate_country column because its of no use
df2 = df2.drop(['consulate_country'], axis = 1)
df2.head()

statisticalValue = (df2['rejection_rate']/100.0).mean()
print('Statistical value is ',statisticalValue)

# Making lists of 0s and 1s based on what the accepted and rejected (uv_unissued) values are
# 0 -> accepted
# 1 -> rejected
FinalList = [] 
for index, row in df2.iterrows():
    numberOfAccepted = row['accepted']
    numberOfRejected = row['uv_unissued']
    arrSize = numberOfAccepted + numberOfRejected
    arr = []
    for i in range(numberOfAccepted):
        arr.append(0)
    for i in range(numberOfRejected):
        arr.append(1)
    FinalList.append(arr)

# Running t test
table = PrettyTable()
table.field_names = ["sch_stat","consulate", "Status", "P-Value", "Statistical Value"]
lengthOfFinalList = len(FinalList)
for i in range(lengthOfFinalList):
    test = FinalList[i]
    tset, pvalue = ttest_1samp(test, statisticalValue)
    if pvalue < 0.05/2:
        table.add_row([df2.loc[i, 'sch_state'],df2.loc[i, 'consulate'], 'x', pvalue, tset])
    else:
        table.add_row([df2.loc[i, 'sch_state'],df2.loc[i, 'consulate'], 'o', pvalue, tset])
key = PrettyTable()
key.field_names = ['symbol', 'meaning']
key.add_row(['x', 'Null hypothesis rejected'])
key.add_row(['o', 'Null hypothesis accepted'])
print(key)
print(table)

"""# India: Hypothesis testing with respect to Multi-Entry Visa Share"""

statisticalValue = (df2['shared_mev_issued']/100.0).mean()
print('Statistical value is ',statisticalValue)

# Adding the non MEV column to my india dataframe
nonMEV = []
for index, row in df2.iterrows():
    value = row['total_uv+mev_issued'] - row['mev_issued']
    nonMEV.append(value)
df2['nonMEV'] = nonMEV
df2.head()

# Making lists of 0s and 1s based on what the MEV issued and nonMEV values are
# 1 -> MEV
# 0 -> nonMEV
FinalList = [] 
for index, row in df2.iterrows():
    numberOfAccepted = row['mev_issued']
    numberOfRejected = row['nonMEV']
    arrSize = numberOfAccepted + numberOfRejected
    arr = []
    for i in range(numberOfAccepted):
        arr.append(1)
    for i in range(numberOfRejected):
        arr.append(0)
    FinalList.append(arr)

# Running t test
table = PrettyTable()

table.field_names = ["sch_stat","consulate", "Status", "P-Value", "Statistical Value"]
lengthOfFinalList = len(FinalList)
for i in range(lengthOfFinalList):
    test = FinalList[i]
    tset, pvalue = ttest_1samp(test, statisticalValue)
    if pvalue < 0.05/2:
        table.add_row([df2.loc[i, 'sch_state'],df2.loc[i, 'consulate'], 'x', pvalue, tset])
    else:
        table.add_row([df2.loc[i, 'sch_state'],df2.loc[i, 'consulate'], 'o', pvalue, tset])
key = PrettyTable()
key.field_names = ['symbol', 'meaning']
key.add_row(['x', 'Null hypothesis rejected'])
key.add_row(['o', 'Null hypothesis accepted'])
print(key)
print(table)

"""# India: Hypothesis testing with respect to Weighted Score"""

weightedScore = []
for index, row in df2.iterrows():
    value = (1- (row['rejection_rate']/100.0))*(row['shared_mev_issued']/100.0)
    weightedScore.append(value)
df2['weighted_score'] = weightedScore

df2.head()

x = (df2.loc[:,['weighted_score']]).values
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
df2['weighted_score'] = pd.DataFrame(x_scaled)

statisticalValue = df2['weighted_score'].mean()
statisticalValue

accepted3 = []
rejected3 = []
for index, row in df2.iterrows():
    value = row['weighted_score']
    accepted3.append(value*row['decisions'])
    rejected3.append((1-value)*row['decisions'])

df2['accepted3'] = accepted3
df2['rejected3'] = rejected3
df2['accepted3'] = df2['accepted3'].astype(int)
df2['rejected3'] = df2['rejected3'].astype(int)
df2.head()

FinalList = [] 
for index, row in df2.iterrows():
    numberOfAccepted = row['accepted3']
    numberOfRejected = row['rejected3']
    arrSize = numberOfAccepted + numberOfRejected
    arr = []
    for i in range(numberOfAccepted):
        arr.append(1)
    for i in range(numberOfRejected):
        arr.append(0)
    FinalList.append(arr)

# Running t test
table = PrettyTable()

table.field_names = ["sch_state","consulate", "Status", "P-Value", "Statistical Value"]
lengthOfFinalList = len(FinalList)
for i in range(lengthOfFinalList):
    test = FinalList[i]
    tset, pvalue = ttest_1samp(test, statisticalValue)
    if pvalue < 0.05/2:
        table.add_row([df2.loc[i, 'sch_state'],df2.loc[i, 'consulate'], 'x', pvalue, tset])
    else:
        table.add_row([df2.loc[i, 'sch_state'],df2.loc[i, 'consulate'], 'o', pvalue, tset])
key = PrettyTable()
key.field_names = ['symbol', 'meaning']
key.add_row(['x', 'Null hypothesis rejected'])
key.add_row(['o', 'Null hypothesis accepted'])
print(key)
print(table)

"""# India: Final Analysis

In my opinion, null hypothesis was rejected in India due to the fact that every year millions of indians apply for visa as you can see in the link: https://qz.com/india/1603047/schengen-countries-list-that-indians-are-crazy-about/

This leads to a longer processing and waiting time which mostly people don't want to go through it and, therefore, tend to do it the illegal way or a substitute way. This is where visa shopping comes into play.

# Japan
"""

df3 = df[df['consulate_country'] == 'JAPAN'] 
df3.head()

"""# Japan: Pie Chart"""

# doing groupby so i can make piecharts using it
df3b = df3.groupby(['sch_state']).sum()
df3b

# making piechart
df3b.plot.pie(y='decisions', figsize=(50,40), fontsize = 14)

df3c = df3.groupby(['consulate']).sum()
df3c

df3c.plot.pie(y='decisions', figsize=(10,30), fontsize = 14)

"""# Japan: Hypothesis testing with respect to Rejection Rate"""

# Adding the accepted column to my japan dataframe
Accepted = []
for index, row in df3.iterrows():
    value = row['total_uv+mev_issued'] + row['LTVs_issued']
    Accepted.append(value)
df3['accepted'] = Accepted
df3.head()

# Dropping the consulate_country column because its of no use
df3 = df3.drop(['consulate_country'], axis = 1)
df3.head()

df3.reset_index(drop = True, inplace = True)
df3.head()

statisticalValue = (df3['rejection_rate']/100.0).mean()
print('Statistical value is ',statisticalValue)

# Making lists of 0s and 1s based on what the accepted and rejected (uv_unissued) values are
# 0 -> accepted
# 1 -> rejected
FinalList = [] 
for index, row in df3.iterrows():
    numberOfAccepted = row['accepted']
    numberOfRejected = row['uv_unissued']
    arrSize = numberOfAccepted + numberOfRejected
    arr = []
    for i in range(numberOfAccepted):
        arr.append(0)
    for i in range(numberOfRejected):
        arr.append(1)
    FinalList.append(arr)

# Running t test
table = PrettyTable()
table.field_names = ["sch_stat","consulate", "Status", "P-Value", "Statistical Value"]
lengthOfFinalList = len(FinalList)
for i in range(lengthOfFinalList):
    test = FinalList[i]
    tset, pvalue = ttest_1samp(test, statisticalValue)
    if pvalue < 0.05/2:
        table.add_row([df3.loc[i, 'sch_state'],df3.loc[i, 'consulate'], 'x', pvalue, tset])
    else:
        table.add_row([df3.loc[i, 'sch_state'],df3.loc[i, 'consulate'], 'o', pvalue, tset])
key = PrettyTable()
key.field_names = ['symbol', 'meaning']
key.add_row(['x', 'Null hypothesis rejected'])
key.add_row(['o', 'Null hypothesis accepted'])
print(key)
print(table)

"""# Japan: Hypothesis testing with respect to Multi-Entry Visa Share"""

MEVrate = []
for index, row in df3.iterrows():
    value = row['mev_issued']*100/row['total_uv+mev_issued']
    MEVrate.append(value)
df3['MEV_rate'] = MEVrate
df3.head()

# using ratio of sum of mev_issued with respect to sum of total_uv+mev_issued as my statistical value
statisticalValue = (df3['shared_mev_issued']/100.0).mean()
print('Statistical value is ',statisticalValue)

# Adding the non MEV column to my india dataframe
nonMEV = []
for index, row in df3.iterrows():
    value = row['total_uv+mev_issued'] - row['mev_issued']
    nonMEV.append(value)
df3['nonMEV'] = nonMEV
df3.head()

# Making lists of 0s and 1s based on what the MEV issued and nonMEV values are
# 1 -> MEV
# 0 -> nonMEV
FinalList = [] 
for index, row in df3.iterrows():
    numberOfAccepted = row['mev_issued']
    numberOfRejected = row['nonMEV']
    arrSize = numberOfAccepted + numberOfRejected
    arr = []
    for i in range(numberOfAccepted):
        arr.append(1)
    for i in range(numberOfRejected):
        arr.append(0)
    FinalList.append(arr)

# Running t test
table = PrettyTable()

table.field_names = ["sch_stat","consulate", "Status", "P-Value", "Statistical Value"]
lengthOfFinalList = len(FinalList)
for i in range(lengthOfFinalList):
    test = FinalList[i]
    tset, pvalue = ttest_1samp(test, statisticalValue)
    if pvalue < 0.05/2:
        table.add_row([df3.loc[i, 'sch_state'],df3.loc[i, 'consulate'], 'x', pvalue, tset])
    else:
        table.add_row([df3.loc[i, 'sch_state'],df3.loc[i, 'consulate'], 'o', pvalue, tset])
key = PrettyTable()
key.field_names = ['symbol', 'meaning']
key.add_row(['x', 'Null hypothesis rejected'])
key.add_row(['o', 'Null hypothesis accepted'])
print(key)
print(table)

"""# Japan: Hypothesis testing with respect to Weighted Score"""

weightedScore = []
for index, row in df3.iterrows():
    value = (1- (row['rejection_rate']/100.0))*(row['shared_mev_issued']/100.0)
    weightedScore.append(value)
df3['weighted_score'] = weightedScore

df3.head()

x = (df3.loc[:,['weighted_score']]).values
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
df3['weighted_score'] = pd.DataFrame(x_scaled)

statisticalValue = df3['weighted_score'].mean()
statisticalValue

accepted3 = []
rejected3 = []
for index, row in df3.iterrows():
    value = row['weighted_score']
    accepted3.append(value*row['decisions'])
    rejected3.append((1-value)*row['decisions'])

df3['accepted3'] = accepted3
df3['rejected3'] = rejected3
df3['accepted3'] = df3['accepted3'].astype(int)
df3['rejected3'] = df3['rejected3'].astype(int)
df3.head()

FinalList = [] 
for index, row in df3.iterrows():
    numberOfAccepted = row['accepted3']
    numberOfRejected = row['rejected3']
    arrSize = numberOfAccepted + numberOfRejected
    arr = []
    for i in range(numberOfAccepted):
        arr.append(1)
    for i in range(numberOfRejected):
        arr.append(0)
    FinalList.append(arr)

# Running t test
table = PrettyTable()

table.field_names = ["sch_state","consulate", "Status", "P-Value", "Statistical Value"]
lengthOfFinalList = len(FinalList)
for i in range(lengthOfFinalList):
    test = FinalList[i]
    tset, pvalue = ttest_1samp(test, statisticalValue)
    if pvalue < 0.05/2:
        table.add_row([df3.loc[i, 'sch_state'],df3.loc[i, 'consulate'], 'x', pvalue, tset])
    else:
        table.add_row([df3.loc[i, 'sch_state'],df3.loc[i, 'consulate'], 'o', pvalue, tset])
key = PrettyTable()
key.field_names = ['symbol', 'meaning']
key.add_row(['x', 'Null hypothesis rejected'])
key.add_row(['o', 'Null hypothesis accepted'])
print(key)
print(table)

"""# Japan: Final Analysis

There are quite a few rejectioned null hypothesis in Japan. This could be somewhat be connected to the strict visa laws that have been applied or the low number of applications sent from japan. Japanese tend to avoid going and working outside of their country mainly due to the language and culture barrier between japan and other countries.
"""